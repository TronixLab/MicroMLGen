{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyML - GaussianNB (Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 - Hardcode Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset is a classic dataset in the field of machine learning and statistics. It was introduced by Sir Ronald A. Fisher in 1936 as an example of discriminant analysis. The dataset is often used for educational purposes and is a common starting point for the practice of pattern classification.\n",
    "\n",
    "\n",
    "Attributes:\n",
    "\n",
    "- Sepal length (in centimeters)\n",
    "\n",
    "- Sepal width (in centimeters)\n",
    "\n",
    "- Petal length (in centimeters)\n",
    "\n",
    "- Petal width (in centimeters)\n",
    "\n",
    "Species:\n",
    "\n",
    "- 0 - Setosa\n",
    "\n",
    "- 1 - Versicolor\n",
    "\n",
    "- 2 - Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "\n",
    "# Add target variable to the DataFrame\n",
    "df['target'] = data.target\n",
    "\n",
    "# Remove NaN values\n",
    "df = df.dropna(axis='rows') #remove NaN\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(pd.unique(df[df.columns[-1]]), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()\n",
    "nrow,ncol = df.shape\n",
    "y = data[:,-1]\n",
    "X = data[:,0:ncol-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Normalization of the data, in order to avoid the effect of the scale of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processed data:')\n",
    "print('Mean: ', np.mean(X, axis = 0))\n",
    "print('STD:', np.std(X, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Bayesian classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the parametric case, assuming that each variable is distributed according to a Normal distribution. Other distributions can also be used.\n",
    "\n",
    "We have already selected the training and test sets. In the training set, we will calculate the mean and standard deviation of each attribute for each class. Next, we classify the data using Bayesian decision theory, i.e.: $X \\in C_i$ if, and only if, $P(C_i|X) = \\max P(C_j|X)$ for all $j$.\n",
    "\n",
    "\n",
    "\n",
    "First, we define a function to calculate the joint probability density: $$p(\\vec{x}|C_i) = \\prod_{j=1}^d p(x_j|C_i), \\quad i=1,\\ldots, k$$ \n",
    "where $C_i$ are the classes. If the distribution is normal, each attribute $X_j$ has the following associated probability density function, for each class:\n",
    "$$\n",
    "p(x_j|C_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_{C_i}}}\\exp \\left[ -\\frac{1}{2}\\left( \\frac{x_j-\\mu_{C_i}}{\\sigma_{C_i}}\\right)^2 \\right], \\quad i=1,2,\\ldots, k.\n",
    "$$\n",
    "Thus, we have defined a function to calculate the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# P - stores the probability of belonging to each class\n",
    "P = pd.DataFrame(data=np.zeros((X_test.shape[0], len(classes))), columns = classes) \n",
    "\n",
    "# Pc - stores the chances of belonging to each class\n",
    "Pc = np.zeros(len(classes)) #fraction of elements in each class\n",
    "\n",
    "# For each \"i\" being a class\n",
    "for i in np.arange(0, len(classes)):\n",
    "    # select the elements of class \"i\"\n",
    "    elements = tuple(np.where(y_train == classes[i]))\n",
    "    # Calculates the probability of belonging to class \"i\"\n",
    "    Pc[i] = len(elements)/len(y_train)\n",
    "    # Selects the elements belonging to class \"i\" in the training set\n",
    "    Z = X_train[elements,:][0]\n",
    "    # Calculates the average of the previously selected values\n",
    "    m = np.mean(Z, axis = 0)\n",
    "    # Calculates covariance matrix of previously selected elements\n",
    "    cv = np.cov(np.transpose(Z))\n",
    "    # Calculates the chance of belonging to each class for the test set\n",
    "    for j in np.arange(0,X_test.shape[0]):\n",
    "        x = X_test[j,:]\n",
    "        # Likelihood function\n",
    "        pj = multivariate_normal.pdf(x, mean=m, cov=cv, allow_singular=True)\n",
    "        P[classes[i]][j] = pj*Pc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "# For each row, calculate the column with the highest probability\n",
    "for i in np.arange(0, X_test.shape[0]):\n",
    "    c = np.argmax(np.array(P.iloc[[i]]))\n",
    "    y_pred.append(classes[c])\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print('Accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 - Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install micromlgen  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromlgen import port\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset is a classic dataset in the field of machine learning and statistics. It was introduced by Sir Ronald A. Fisher in 1936 as an example of discriminant analysis. The dataset is often used for educational purposes and is a common starting point for the practice of pattern classification.\n",
    "\n",
    "\n",
    "Attributes:\n",
    "\n",
    "- Sepal length (in centimeters)\n",
    "\n",
    "- Sepal width (in centimeters)\n",
    "\n",
    "- Petal length (in centimeters)\n",
    "\n",
    "- Petal width (in centimeters)\n",
    "\n",
    "Species:\n",
    "\n",
    "- 0 - Setosa\n",
    "\n",
    "- 1 - Versicolor\n",
    "\n",
    "- 2 - Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (150, 4)\n",
      "Target variable shape:  (150,)\n"
     ]
    }
   ],
   "source": [
    "print('Input shape: ', X.shape)\n",
    "print('Target variable shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Create the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - Evaluating the model with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predict = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(training_predict, y_train)\n",
    "print('Accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        31\n",
      "           1      0.919     0.919     0.919        37\n",
      "           2      0.919     0.919     0.919        37\n",
      "\n",
      "    accuracy                          0.943       105\n",
      "   macro avg      0.946     0.946     0.946       105\n",
      "weighted avg      0.943     0.943     0.943       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, training_predict, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31  0  0]\n",
      " [ 0 34  3]\n",
      " [ 0  3 34]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_train, training_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 - Evaluating the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(test_predict, y_test)\n",
    "print('Accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.923     0.960        13\n",
      "           2      0.929     1.000     0.963        13\n",
      "\n",
      "    accuracy                          0.978        45\n",
      "   macro avg      0.976     0.974     0.974        45\n",
      "weighted avg      0.979     0.978     0.978        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, test_predict, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 - Obtaining the model to be implemented in the microcontroller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pragma once\n",
      "#include <cstdarg>\n",
      "namespace Eloquent {\n",
      "    namespace ML {\n",
      "        namespace Port {\n",
      "            class GaussianNB {\n",
      "                public:\n",
      "                    /**\n",
      "                    * Predict class for features vector\n",
      "                    */\n",
      "                    int predict(float *x) {\n",
      "                        float votes[3] = { 0.0f };\n",
      "                        float theta[4] = { 0 };\n",
      "                        float sigma[4] = { 0 };\n",
      "                        theta[0] = 4.964516129032; theta[1] = 3.377419354839; theta[2] = 1.464516129032; theta[3] = 0.248387096774;\n",
      "                        sigma[0] = 0.111966704288; sigma[1] = 0.136586891592; sigma[2] = 0.033257026868; sigma[3] = 0.011529659543;\n",
      "                        votes[0] = 0.295238095238 - gauss(x, theta, sigma);\n",
      "                        theta[0] = 5.862162162162; theta[1] = 2.724324324324; theta[2] = 4.210810810811; theta[3] = 1.302702702703;\n",
      "                        sigma[0] = 0.275325057719; sigma[1] = 0.087246168019; sigma[2] = 0.239342588764; sigma[3] = 0.041344049684;\n",
      "                        votes[1] = 0.352380952381 - gauss(x, theta, sigma);\n",
      "                        theta[0] = 6.559459459459; theta[1] = 2.986486486486; theta[2] = 5.545945945946; theta[3] = 2.005405405405;\n",
      "                        sigma[0] = 0.422410521562; sigma[1] = 0.096303874374; sigma[2] = 0.288429513527; sigma[3] = 0.085916730473;\n",
      "                        votes[2] = 0.352380952381 - gauss(x, theta, sigma);\n",
      "                        // return argmax of votes\n",
      "                        uint8_t classIdx = 0;\n",
      "                        float maxVotes = votes[0];\n",
      "\n",
      "                        for (uint8_t i = 1; i < 3; i++) {\n",
      "                            if (votes[i] > maxVotes) {\n",
      "                                classIdx = i;\n",
      "                                maxVotes = votes[i];\n",
      "                            }\n",
      "                        }\n",
      "\n",
      "                        return classIdx;\n",
      "                    }\n",
      "\n",
      "                protected:\n",
      "                    /**\n",
      "                    * Compute gaussian value\n",
      "                    */\n",
      "                    float gauss(float *x, float *theta, float *sigma) {\n",
      "                        float gauss = 0.0f;\n",
      "\n",
      "                        for (uint16_t i = 0; i < 4; i++) {\n",
      "                            gauss += log(sigma[i]);\n",
      "                            gauss += abs(x[i] - theta[i]) / sigma[i];\n",
      "                        }\n",
      "\n",
      "                        return gauss;\n",
      "                    }\n",
      "                };\n",
      "            }\n",
      "        }\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "print(port(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 - Saves the template in a .h file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./GaussianNB/GaussianNB.h', 'w') as file:   \n",
    "    file.write(port(model, classname='GaussianNB'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
